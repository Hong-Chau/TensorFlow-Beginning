{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_batch(X, X_, size):\n",
    "    a = np.random.choice(len(X), size, replace=False)\n",
    "    return X[a], X_[a]\n",
    "\n",
    "\n",
    "def noise_validator(noise, allowed_noises):\n",
    "    '''Validates the noise provided'''\n",
    "    try:\n",
    "        if noise in allowed_noises:\n",
    "            return True\n",
    "        elif noise.split('-')[0] == 'mask' and float(noise.split('-')[1]):\n",
    "            t = float(noise.split('-')[1])\n",
    "            if t >= 0.0 and t <= 1.0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    except:\n",
    "        return False\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "allowed_activations = ['sigmoid', 'tanh', 'softmax', 'relu', 'linear']\n",
    "allowed_noises = [None, 'gaussian', 'mask']\n",
    "allowed_losses = ['rmse', 'cross-entropy']\n",
    "\n",
    "\n",
    "class StackedAutoEncoder:\n",
    "    \"\"\"A deep autoencoder with denoising capability\"\"\"\n",
    "\n",
    "    def assertions(self):\n",
    "        global allowed_activations, allowed_noises, allowed_losses\n",
    "        assert self.loss in allowed_losses, 'Incorrect loss given'\n",
    "        assert 'list' in str(\n",
    "            type(self.dims)), 'dims must be a list even if there is one layer.'\n",
    "        assert len(self.epoch) == len(\n",
    "            self.dims), \"No. of epochs must equal to no. of hidden layers\"\n",
    "        assert len(self.activations) == len(\n",
    "            self.dims), \"No. of activations must equal to no. of hidden layers\"\n",
    "        assert all(\n",
    "            True if x > 0 else False\n",
    "            for x in self.epoch), \"No. of epoch must be atleast 1\"\n",
    "        assert set(self.activations + allowed_activations) == set(\n",
    "            allowed_activations), \"Incorrect activation given.\"\n",
    "        assert noise_validator(\n",
    "            self.noise, allowed_noises), \"Incorrect noise given\"\n",
    "\n",
    "    def __init__(self, dims, activations, epoch=1000, noise=None, loss='rmse',\n",
    "                 lr=0.001, batch_size=100, print_step=50):\n",
    "        self.print_step = print_step\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.loss = loss\n",
    "        self.activations = activations\n",
    "        self.noise = noise\n",
    "        self.epoch = epoch\n",
    "        self.dims = dims\n",
    "        self.assertions()\n",
    "        self.depth = len(dims)\n",
    "        self.weights, self.biases = [], []\n",
    "\n",
    "    def add_noise(self, x):\n",
    "        if self.noise == 'gaussian':\n",
    "            n = np.random.normal(0, 0.1, (len(x), len(x[0])))\n",
    "            return x + n\n",
    "        if 'mask' in self.noise:\n",
    "            frac = float(self.noise.split('-')[1])\n",
    "            temp = np.copy(x)\n",
    "            for i in temp:\n",
    "                n = np.random.choice(len(i), round(\n",
    "                    frac * len(i)), replace=False)\n",
    "                i[n] = 0\n",
    "            return temp\n",
    "        if self.noise == 'sp':\n",
    "            pass\n",
    "\n",
    "    def fit(self, x):\n",
    "        for i in range(self.depth):\n",
    "            print('Layer {0}'.format(i + 1))\n",
    "            if self.noise is None:\n",
    "                x = self.run(data_x=x, activation=self.activations[i],\n",
    "                             data_x_=x,\n",
    "                             hidden_dim=self.dims[i], epoch=self.epoch[\n",
    "                                 i], loss=self.loss,\n",
    "                             batch_size=self.batch_size, lr=self.lr,\n",
    "                             print_step=self.print_step)\n",
    "            else:\n",
    "                temp = np.copy(x)\n",
    "                x = self.run(data_x=self.add_noise(temp),\n",
    "                             activation=self.activations[i], data_x_=x,\n",
    "                             hidden_dim=self.dims[i],\n",
    "                             epoch=self.epoch[\n",
    "                                 i], loss=self.loss,\n",
    "                             batch_size=self.batch_size,\n",
    "                             lr=self.lr, print_step=self.print_step)\n",
    "\n",
    "    def transform(self, data):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        x = tf.constant(data, dtype=tf.float32)\n",
    "        for w, b, a in zip(self.weights, self.biases, self.activations):\n",
    "            weight = tf.constant(w, dtype=tf.float32)\n",
    "            bias = tf.constant(b, dtype=tf.float32)\n",
    "            layer = tf.matmul(x, weight) + bias\n",
    "            x = self.activate(layer, a)\n",
    "        return x.eval(session=sess)\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.fit(x)\n",
    "        return self.transform(x)\n",
    "\n",
    "    def run(self, data_x, data_x_, hidden_dim, activation, loss, lr,\n",
    "            print_step, epoch, batch_size=100):\n",
    "        tf.reset_default_graph()\n",
    "        input_dim = len(data_x[0])\n",
    "        sess = tf.Session()\n",
    "        x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim], name='x')\n",
    "        x_ = tf.placeholder(dtype=tf.float32, shape=[\n",
    "                            None, input_dim], name='x_')\n",
    "        encode = {'weights': tf.Variable(tf.truncated_normal(\n",
    "            [input_dim, hidden_dim], dtype=tf.float32)),\n",
    "            'biases': tf.Variable(tf.truncated_normal([hidden_dim],\n",
    "                                                      dtype=tf.float32))}\n",
    "        decode = {'biases': tf.Variable(tf.truncated_normal([input_dim],\n",
    "                                                            dtype=tf.float32)),\n",
    "                  'weights': tf.transpose(encode['weights'])}\n",
    "        encoded = self.activate(\n",
    "            tf.matmul(x, encode['weights']) + encode['biases'], activation)\n",
    "        decoded = tf.matmul(encoded, decode['weights']) + decode['biases']\n",
    "\n",
    "        # reconstruction loss\n",
    "        if loss == 'rmse':\n",
    "            loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(x_, decoded))))\n",
    "        elif loss == 'cross-entropy':\n",
    "            loss = -tf.reduce_mean(x_ * tf.log(decoded))\n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(epoch):\n",
    "            b_x, b_x_ = get_batch(\n",
    "                data_x, data_x_, batch_size)\n",
    "            sess.run(train_op, feed_dict={x: b_x, x_: b_x_})\n",
    "            if (i + 1) % print_step == 0:\n",
    "                l = sess.run(loss, feed_dict={x: data_x, x_: data_x_})\n",
    "                print('epoch {0}: global loss = {1}'.format(i, l))\n",
    "        # debug\n",
    "        # print('Decoded', sess.run(decoded, feed_dict={x: self.data_x_})[0])\n",
    "        self.weights.append(sess.run(encode['weights']))\n",
    "        self.biases.append(sess.run(encode['biases']))\n",
    "        return sess.run(encoded, feed_dict={x: data_x_})\n",
    "\n",
    "    def activate(self, linear, name):\n",
    "        if name == 'sigmoid':\n",
    "            return tf.nn.sigmoid(linear, name='encoded')\n",
    "        elif name == 'softmax':\n",
    "            return tf.nn.softmax(linear, name='encoded')\n",
    "        elif name == 'linear':\n",
    "            return linear\n",
    "        elif name == 'tanh':\n",
    "            return tf.nn.tanh(linear, name='encoded')\n",
    "        elif name == 'relu':\n",
    "            return tf.nn.relu(linear, name='encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "data, target = mnist.train.images, mnist.train.labels\n",
    "\n",
    "# train / test  split\n",
    "idx = np.random.rand(data.shape[0]) < 0.8\n",
    "train_X, train_Y = data[idx], target[idx]\n",
    "test_X, test_Y = data[~idx], target[~idx]\n",
    "\n",
    "model = StackedAutoEncoder(dims=[200, 200], activations=['linear', 'linear'], epoch=[\n",
    "                           3000, 3000], loss='rmse', lr=0.007, batch_size=100, print_step=200)\n",
    "model.fit(train_X)\n",
    "test_X_ = model.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(~idx & idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "data, target = mnist.train.images, mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train / test  split\n",
    "idx = np.random.rand(data.shape[0]) < 0.8\n",
    "train_X, train_Y = data[idx], target[idx]\n",
    "test_X, test_Y = data[~idx], target[~idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = StackedAutoEncoder(dims=[100, 200], activations=['linear', 'linear'], epoch=[\n",
    "                           3000, 3000], loss='rmse', lr=0.007, batch_size=100, print_step=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1\n",
      "epoch 199: global loss = 11.06434440612793\n",
      "epoch 399: global loss = 4.651761531829834\n",
      "epoch 599: global loss = 2.252706527709961\n",
      "epoch 799: global loss = 1.1503130197525024\n",
      "epoch 999: global loss = 0.6233928799629211\n",
      "epoch 1199: global loss = 0.39355969429016113\n",
      "epoch 1399: global loss = 0.3077891767024994\n",
      "epoch 1599: global loss = 0.2778095304965973\n",
      "epoch 1799: global loss = 0.24237558245658875\n",
      "epoch 1999: global loss = 0.19929556548595428\n",
      "epoch 2199: global loss = 0.16713203489780426\n",
      "epoch 2399: global loss = 0.14163798093795776\n",
      "epoch 2599: global loss = 0.1220889464020729\n",
      "epoch 2799: global loss = 0.10842537879943848\n",
      "epoch 2999: global loss = 0.09799296408891678\n",
      "Layer 2\n",
      "epoch 199: global loss = 13.9861421585083\n",
      "epoch 399: global loss = 3.550126075744629\n",
      "epoch 599: global loss = 0.9688515067100525\n",
      "epoch 799: global loss = 0.21573682129383087\n",
      "epoch 999: global loss = 0.06619735062122345\n",
      "epoch 1199: global loss = 0.031778816133737564\n",
      "epoch 1399: global loss = 0.03018585406243801\n",
      "epoch 1599: global loss = 0.02950664795935154\n",
      "epoch 1799: global loss = 0.026361901313066483\n",
      "epoch 1999: global loss = 0.025463948026299477\n",
      "epoch 2199: global loss = 0.02414165809750557\n",
      "epoch 2399: global loss = 0.024051666259765625\n",
      "epoch 2599: global loss = 0.024471284821629524\n",
      "epoch 2799: global loss = 0.020149173215031624\n",
      "epoch 2999: global loss = 0.02104555256664753\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X_ = model.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11013, 200)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11013, 784)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43987, 784)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = len(test_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dims[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
